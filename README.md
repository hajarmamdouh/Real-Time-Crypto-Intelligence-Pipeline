# Real-Time Crypto Intelligence Pipeline

## 1Ô∏è‚É£ Objectif du projet

Ce projet vise √† concevoir et impl√©menter une **pipeline Data Engineering temps r√©el** pour l‚Äôentreprise **INVISTIS**, combinant :

- **Streaming** : Kafka + Spark Structured Streaming  
- **Batch** : Airflow + FRED API  
- **Multi-sink** : Data Lake, Supabase, Kafka (`alerts_topic`), Redis (optionnel)  
- Gestion de sch√©mas et validation end-to-end  
- Pr√™t √† la production pour int√©gration dans l‚Äôapplication backend de gestion de portefeuilles crypto  

---

## 2Ô∏è‚É£ Contexte

INVISTIS souhaite int√©grer des donn√©es crypto temps r√©el pour permettre aux Data Scientists et aux d√©veloppeurs backend de :

- Analyser les fluctuations des prix et sentiments sur le march√© crypto  
- D√©tecter les √©v√©nements de volatilit√© et les anomalies  
- Enrichir leurs mod√®les d‚Äôinvestissement et scoring de portefeuille  

Ce projet fait partie de la mission **DATA NEXT**, pour tester la faisabilit√© et la readiness production de la plateforme.  

---

## 3Ô∏è‚É£ Architecture

### Sources de donn√©es

- **Binance WebSocket** ‚Üí Trades crypto en temps r√©el  
- **NewsAPI** ‚Üí Articles financiers et crypto  
- **FRED API** ‚Üí Indicateurs √©conomiques pour enrichissement  

### Outputs / Destinations

- **Databricks Data Lake** ‚Üí RAW + Enriched (format Parquet)  
- **Supabase** ‚Üí Warehouse layer pour consommation backend  
- **Kafka `alerts_topic`** ‚Üí Alertes sur √©v√©nements importants  
- **Redis (optionnel)** ‚Üí Cache backend pour acc√®s rapide  

### Diagramme d‚Äôarchitecture (ASCII)

```text
         +-------------------+        +-----------------+
         | Binance WebSocket |        |     NewsAPI     |
         +--------+----------+        +--------+--------+
                  |                            |
                  v                            v
          +-------+----------------------------+------+
          |    Spark Structured Streaming (Databricks) |
          |  - Stream-to-Stream Join ¬±5 min            |
          |  - Watermark / Fen√™tres 1min / 5min       |
          +------------+------------+-----------------+
                       |            |
         +-------------+            +-------------+
         |                                          |
+--------v--------+                       +---------v--------+
| Delta Lake RAW  |                       | Supabase Table   |
| & Enriched      |                       | market_enriched |
+-----------------+                       +-----------------+
         |
         v
   +-----+------+
   | Kafka Topic|
   | alerts_topic|
   +-----+------+
         |
         v
      [Redis Cache] (optionnel)

√âtape 4 ‚Äî Databricks (Streaming + Join)

Configuration Spark Structured Streaming avec Kafka :

CONFLUENT_BOOTSTRAP = "pkc-921jm.us-east-2.aws.confluent.cloud:9092"
CONFLUENT_API_KEY   = "JWVBQ7RG25AVCHHY"
CONFLUENT_SECRET    = "cflt9aP2o4QFY2CAugnhB/J/MMhEIDYtyGdj3Gu6dcFih68+sqYqyvBvzj8ABd4g"

Lecture des topics Kafka ‚Üí trades_topic, news_topic

Stream-to-Stream Join ¬±5 min avec watermark

Multi-sink : Delta Lake, Supabase, Kafka alerts_topic

üîó Lien Databricks Notebook

üîπ √âtape 5 ‚Äî Supabase (Warehouse Layer)

Installation package Python :

%pip install supabase

Configuration :

SUPABASE_URL = "https://TON_PROJECT_ID.supabase.co"
SUPABASE_KEY = "TON_ANON_PUBLIC_KEY"
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

Cr√©ation de la table : market_enriched

√âcriture des batch et stream Spark vers Supabase via foreachBatch

üîπ √âtape 6 ‚Äî Data Lake

RAW : /home/hajar_mamdouh/data_lake/raw/fred/

Enriched : /Volumes/invistis/datalake/raw/enriched

üîπ √âtape 7 ‚Äî Validation & Schema

Sch√©ma Kafka pour trades et news d√©fini avec StructType

V√©rification des valeurs nulles, types num√©riques et timestamps

Test de la jointure stream-to-stream et batch ‚Üí OK

6Ô∏è‚É£ R√©sultats

Kafka topics cr√©√©s et fonctionnels

Stream-to-stream join Databricks ex√©cut√©

Multi-sink vers Delta Lake et Supabase confirm√©

Airflow DAG pour FRED fonctionnel

Sch√©mas valid√©s et data flow op√©rationnel

7Ô∏è‚É£ Liens utiles

Confluent Cloud :  
https://confluent.cloud/environments/env-dz5wx1/clusters/lkc-y76m1j/overview?granularity=PT1M&interval=3600000&label=Last%20hour&refresh=60000

Databricks Notebook :  
https://dbc-679742dc-7a67.cloud.databricks.com/editor/notebooks/426871642664109?o=7474646532280705
